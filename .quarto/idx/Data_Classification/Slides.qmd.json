{"title":"Classifying Data","markdown":{"yaml":{"format":{"revealjs":{"theme":"serif","code-fold":true,"controls":true,"navigation-mode":"linear","controls-layout":"bottom-right","controls-tutorial":true,"slide-number":true,"show-slide-number":"all","pdfMaxPagesPerSlide":1}},"fig-dpi":300},"headingText":"Classifying Data","containsRefs":false,"markdown":"\n\n\n\n\n\n::::{.columns}\n:::{.column width=\"100%\"}\n\nFinding patterns in complex datasets, emphasizing aspects we think are important.\n:::\n::::\n\n\n\n\nDescriptive Statistics\n----------------------\n\n\n::::{.columns}\n:::{.column width=\"100%\"}\n\n\n\n\nDescriptors by data type| **Statistic** | **Nominal** | **Ordinal** | **Interval** | **Ratio** |\n| Equality | x | x | x | x |\n| Counts/Mode | x | x | x | x |\n| Rank/Order |  | x | x | x |\n| Median |  | ~ | x | x |\n| Add/Subtract |  |  | x | x |\n| Mean |  |  | x | x |\n| Multiply/Divide |  |  |  | x |\n\n\n\n\n:::\n::::\n\n\n\n\nMeasures of Central Tendency\n----------------------------\n\n\n::::{.columns}\n:::{.column width=\"100%\"}\n\n\nHighlight the \"central\" feature in a dataset.\n\n\n* **Mode**: The most frequent value in a set\n* **Median**: The middle value in a set\n\t+ Data is ranked to find the center point\n\t\n\t\t- 50% above, 50% below+ Not impacted by outliers\n* **Mean**: The sum of all values divided by the number of values\n\t+ Impacted by outliers\n\n\n\n\n:::\n::::\n\n\n\n\nMeasures of Dispersion\n----------------------\n\n\n::::{.columns}\n:::{.column width=\"100%\"}\n\n\nThese statistics give context a measure of central tendency.\n\n\n* **Range**: The difference between the maximum and minimum values\n* **Inter-quartile Range**: Difference between 75th and 25th percentile value\n+ Spread around the median (50th percentile), not influenced by outliers\n\n* **Standard Deviation**: $\\sigma = \\sqrt{\\frac{1}{N}\\sum\\_{i=1}^n (x\\_i - U)^2}$\n+ Spread of data values (x) around the mean(U), influenced by outliers\n\n\n\n\n:::\n::::\n\n\n\n\nFrequency Distribution\n----------------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nFrequency of occurrence in a **qualitative** data.\n\n\n* Counts per category\n* Bar charts are a useful tool for visualizing frequency distributions\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/Frequency_Dist.svg)\n\n\n\n\n:::\n::::\n\n\n\n\nProbability Distribution\n------------------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nProbability of occurrence in a **quantitative** dataset.\n\n\n* **Normal Distribution**: idealized, based on distance from the mean in **standard deviations**.\n+ Assumed distribution in many statistical tests.\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n![](images/normal_dist.svg)\n\n\n\n\n|  |  |\n| --- | --- |\n| ±1 $\\sigma$ | 68% of observations |\n| ±2 $\\sigma$ | 95% of observations |\n| ±3 $\\sigma$ | 99.7% of observations |\n\n\n\n\n:::\n::::\n\n\n\n\nHistograms\n----------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nUseful for **quantitative** data.\n\n\n* Orders data and groups data into **bins** of consistent width\n* Helpful for:\n+ Approximating probability distribution\n+ Grouping data into classes\n+ Outlier detection\n\n* Similar to bar charts, **but** not the same thing!\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/histogram.png)\n\n\n\n\n:::\n::::\n\n\n\n\nDeviating from the Norm\n-----------------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nData rarely fits a normal distribution perfectly:\n\n\n* **Skew**: deviates from a normal distribution\n+ Tails with outliers\n\n* **Kurtosis**: deviates from a normal distribution\n+ Dispersed or clustered\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\nNear Normal\n\n\n![](images/normal_dist.png)\n\n\n\n\nSkewed Normal\n\n\n![](images/normal_dist2.png)\n\n\n\n\nHighly Skewed\n\n\n![](images/normal_dist3.png)\n\n\n\n\n\n:::\n::::\n\n\n\n\nTopHat Question 1\n=================\n\n\n::::{.columns}\n:::{.column width=\"100%\"}\n\n\nIf you are working with nominal or ordinal data, and you want see how many observations you have for each category, you would use:\n\n\n* A histogram to plot a probability distribution\n* A histogram to plot a frequency distribution\n* A bar chart to plot a probability distribution\n* A bar chart to plot a frequency distribution\n\n\n\n\n:::\n::::\n\n\n\n\nNormalizing Data\n----------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nAllows us to account for confounding variables that mask or hide patterns in our data.\n\n\n* Helpful to scale or normalize a value by a another value - time, area, population etc.\n* Examples:\n+ Income vs. money spent on food\n+ Population vs. shape area\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\nHighly Correlated\n\n\n![](images/confounder_1.png)\n\n\n\n\nNo Correlation\n\n\n![](images/confounder_2.png)\n\n\n\n\n\n:::\n::::\n\n\n\n\nMultiple Confounders\n--------------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nIt isn't always straightforward to account for multiple variables.\n\n\n* ex: COVID rates by age groups\n![](images/horgan.jpg)\n\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n![](images/multiple_confounders.png)\n\n\n1. Population by age group\n2. Workforce participation\n3. Occupational exposure\n\n\n\n\n:::\n::::\n\n\n\n\nStandardizing\n-------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nAlso allow us to compare between **two or more** variables in different units / scales.\n\n\n* $z = \\frac{x-\\overline{U}}{\\sigma}$\n* Similar idea to normalizing, but:\n+ Removing the mean and standard deviation from multiple variables\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/Standardizing.png)\n\n\n\n\n\n:::\n::::\n\n\n\n\nTopHat Question 2\n=================\n\n\n::::{.columns}\n:::{.column width=\"100%\"}\n\n\nWhich of these countries has the highest population density? \\* *Populations and Areas are approximate, given to two significant figures for convenience.*\n\n\n* **Monaco**: Pop (37,000), Area (2 km2)\n* **Singapore**: Pop (5,500,000), Area (720 km2)\n* **China**: Pop (1,400,000,000), Area (9,600,000 km2)\n\n\n\n\n:::\n::::\n\n\n\n\nClassification Methods\n----------------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\n**Unsupervised:**\n\n\n* Data defined classes - the user decides on the number of classes\n* The rest is left of up to an algorithm\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n**Supervised:**\n\n\n* User defined - the user explicitly defines classes\n* Or provides set of classes as training data\n* Degree of user input is variable, more than unsupervised\n\n\n\n\n:::\n::::\n\n\n\n\nCommon Examples in Arc Pro\n--------------------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nVancouver dissemination area populations\n\n\n* Not classified\n* Color scheme is stretched between min/max\n* Difficult to see patterns\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/Unclassified.png)\n\n\n\n\n:::\n::::\n\n\n\n\nEqual Interval\n--------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nOne of the simplest classification schemes.\n\n\n* Data is split to classes of equal width based on the range.\n* **Unsupervised**: user defines number of bins.\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/Equal_Int.png)\n\n\n\n\n:::\n::::\n\n\n\n\nDefined Interval\n----------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nAnother of the simplest classification schemes.\n\n\n* Data is split to classes of equal width based on the range.\n* **Unsupervised**: user defines bin width.\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/Defined.png)\n\n\n\n\n:::\n::::\n\n\n\n\nQuantiles\n---------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nSlightly more complex classification scheme.\n\n\n* Data is split into classes by percentiles.\n+ e.g. 0-20%, 20-40%, ... 80-100%.\n\n* **Unsupervised**: user defines number of bins.\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/Quantile.png)\n\n\n\n\n:::\n::::\n\n\n\n\nNatural Breaks\n--------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nMore complex, data is split using the [Jenks algorithm](http://wiki.gis.com/wiki/index.php/Jenks_Natural_Breaks_Classification).\n\n\n* Optimizes splits, by maximizing within group similarity and between group dissimilarity.\n+ \"Natural\" classes.\n\n* **Unsupervised**: user defines number of bins.\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/Natural_break.png)\n\n\n\n\n:::\n::::\n\n\n\n\nStandard Deviation\n------------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nInformative to \"experts\", but not accessible for all.\n\n\n* Classes based on \"distance\" from the mean in standard deviations.\n+ Unit-less, converts to interval data.\n+ Diverging color maps.\n\n* **Unsupervised**: user defines number of bins.\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/Standard_dev.png)\n\n\n\n\n:::\n::::\n\n\n\n\nManual Breaks\n-------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\n**Supervised**: User defines break values.\n\n\n* Allows us to choose more meaningful break values if necessary.\n* Incorporate multiple factors\n* Influence the way the data is perceived.\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/Natural_break.png)\n\n\n\n\n:::\n::::\n\n\n\n\nTopHat Question 3\n=================\n\n\n::::{.columns}\n:::{.column width=\"100%\"}\n\n\nThis classification method seeks to maximize the similarity between data values within groups and maximize the dissimilarity in data values between groups. It tries to find the \"optimal\" splits within a dataset.\n\n\n* Manual Breaks\n* Quantiles\n* Natural Breaks\n* Equal Interval\n* Standard Deviation\n\n\n\n\n:::\n::::\n\n\n\n\nMore Complex Methods\n--------------------\n\n\n::::{.columns}\n:::{.column width=\"100%\"}\n\n\nThere are many classification methods that are a bit too complex to actually perform in this course.\n\n\n* I'm introducing some because important to be aware of them.\n* You'll encounter them if you continue with GIS.\n\n\n\n\n:::\n::::\n\n\n\n\nK-means\n-------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nAlgorithm uses random steps to group data into clusters.\n\n\n* **Unsupervised**: user defines number of bins & iterations.\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/kmeans.gif)\n\n\n\n\n:::\n::::\n\n\n\n\nMedian Absolute Deviation\n-------------------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nUsed for automated detection of outliers.\n\n\n* **Unsupervised**: user defines error threshold.\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/MAD.jpg)\n\n\n\n\n:::\n::::\n\n\n\n\nDecision Trees\n--------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nFit training data to user defined categories.\n\n\n* **Supervised**: user provides training classes.\n* *Automated*: algorithm determines break values.\n* Risk of over-fitting\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/tree.png)\n\n\n\n\n:::\n::::\n\n\n\n\nRandom Forests\n--------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nMultiple trees (>100) can be averaged to increase performance and generalization.\n\n\n* **Supervised**: user provides training classes and \"hyperparameters\".\n* *Automated*: algorithm determines break values.\n* Low risk of over-fitting\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/forest.gif)\n\n\n\n\n:::\n::::\n\n\n\n\nLandscape Classification\n------------------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nMultiple trees (>100) can be averaged to increase performance and generalization.\n\n\n* **Supervised**: user provides training classes and \"hyperparameters\".\n* *Automated*: algorithm determines break values.\n* Low risk of over-fitting\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/FishIsland_NDVI2.gif)\n\n\n\n\n:::\n::::\n\n\n\n\nNeural Networks\n---------------\n\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n\nOne of the most complex methods.\n\n\n* **Supervised**: user provides training classes and \"hyperparameters\".\n* *Automated*: algorithm maps relationships in dataset.\n* Risk of over-fitting\n+ Requires careful inspection\n\n\n\n:::\n:::{.column width=\"50%\"}\n\n\n\n![](images/NN.gif)\n\n\n\n\n:::\n::::\n\n\n\n\nTopHat Question 4\n=================\n\n\n::::{.columns}\n:::{.column width=\"100%\"}\n\n\nUnsupervised classification methods typically require more user input than supervised classification methods.\n\n\n* True\n* False\n\n\n\n\n:::\n::::\n\n\n"},"formats":{"revealjs":{"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":300,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","output-file":"Slides.html"},"language":{},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.2.313","auto-stretch":true,"theme":"serif","controls":true,"navigationMode":"linear","controlsLayout":"bottom-right","controlsTutorial":true,"slideNumber":true,"showSlideNumber":"all","pdfMaxPagesPerSlide":1}}}}